<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>GPT 1</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat&display=swap" rel="stylesheet">
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body>
<div class="frosted text-center mt-16">
    <h1 class="title mb-8 text-4xl p-2">GPT-1</h1>
    <p class="px-4">Појавувајќи се во 2018 овој модел е трениран на огромна количина на податоци од BookCorpus. BookCorpus e всушност текст кој што се состои од повеќе од 11000 различни книги најдени од интернетот.Овој модел во него има 117 милиони параметри. За разлика од претходните NLP модели користи non-labeled податоци т.е. не се проверени податоците од предвреме. Ова овозможува голема флексибилност во генерирањето на одговори,одговарање на прашања и слично. Неговата архитектура изгледа вака:
        Но, ова комплицирано нешто не треба да го знаеме. Се што треба да знаеме е дека зема го зема секој збор од текстот внесен од корисникот, го трансформира, и потоа имплементира неколку предвреме договорени формули (Linear, па потоа SoftMax) врз трансофрмираниот збор од текстот. На крај се прикажува новогенерираниот текст. Како резултат на овој начин на тренирање GPT-1 има можност да изведува zero-shot performance врз различни задачи. Ова означува голем успех кај овие GPT модели, бидејќи докажува дека овој модел на NLP може да се генерализира. Користејќи transfer learning како основа GPT бргу  станува способен да извршува задачи поврзани со јазик со многу малку надгледување или пак поправање. Со овој модел се отворени вратите за други многу пософистицирани модели.
    </p>
</div>
</body>
</html>